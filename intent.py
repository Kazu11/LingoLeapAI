# -*- coding: utf-8 -*-
"""Done.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ibh9osVW7H0S0uqsJeMF7_mDBcA_Ryfm

# Intent Recognition with BERT using Keras and TensorFlow 2
"""

!nvidia-smi

!pip install tensorflow-gpu >> /dev/null

!pip install --upgrade grpcio >> /dev/null

!pip install tqdm  >> /dev/null

!pip install bert-for-tf2 >> /dev/null

!pip install sentencepiece >> /dev/null

# Commented out IPython magic to ensure Python compatibility.
import os
import math
import datetime

from tqdm import tqdm

import pandas as pd
import numpy as np

import tensorflow as tf
from tensorflow import keras

import bert
from bert import BertModelLayer
from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights
from bert.tokenization.bert_tokenization import FullTokenizer

import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from matplotlib import rc

from sklearn.metrics import confusion_matrix, classification_report

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

rcParams['figure.figsize'] = 12, 8

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

"""# Data

The data contains various user queries categorized into seven intents. It is hosted on [GitHub](https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines) and is first presented in [this paper](https://arxiv.org/abs/1805.10190).
"""

!gdown --id 1G28KR1MPB6gd2daB3jC07ZQ1RLSW9B7L --output train.csv
!gdown --id 1_7SWf8z5uwcmgZrhBb1e-YDXgjIsK9VL --output valid.csv
!gdown --id 198wgBY87WiLI0zwB9YbkXR_UTkDtx6WJ --output test.csv

train = pd.read_csv("train.csv")
valid = pd.read_csv("valid.csv")
test = pd.read_csv("test.csv")

train = pd.concat([train, valid], ignore_index=True)

train.shape

train.head()

chart = sns.countplot(train.intent, palette=HAPPY_COLORS_PALETTE)
plt.title("Number of texts per intent")
chart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');

"""# Intent Recognition with BERT"""

!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip

!unzip uncased_L-12_H-768_A-12.zip

os.makedirs("model", exist_ok=True)

!mv uncased_L-12_H-768_A-12/ model

bert_model_name="uncased_L-12_H-768_A-12"

bert_ckpt_dir = os.path.join("model/", bert_model_name)
bert_ckpt_file = os.path.join(bert_ckpt_dir, "bert_model.ckpt")
bert_config_file = os.path.join(bert_ckpt_dir, "bert_config.json")

"""## Preprocessing"""

class IntentDetectionData:
    DATA_COLUMN = "text"
    LABEL_COLUMN = "intent"

    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):
        self.tokenizer = tokenizer
        self.max_seq_len = 0
        self.classes = classes

        train, test = map(lambda df: df.reindex(df[IntentDetectionData.DATA_COLUMN].str.len().sort_values().index), [train, test])

        ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])

        print("max seq_len", self.max_seq_len)
        self.max_seq_len = min(self.max_seq_len, max_seq_len)
        self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])

    def _prepare(self, df):
        x, y = [], []

        for _, row in tqdm(df.iterrows()):
            text, label = row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]
            tokens = self.tokenizer.tokenize(text)
            tokens = ["[CLS]"] + tokens + ["[SEP]"]
            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
            self.max_seq_len = max(self.max_seq_len, len(token_ids))
            x.append(token_ids)
            y.append(self.classes.index(label))

        return x, y

    def _pad(self, ids):
        x, masks = [], []
        for input_ids in ids:
            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]
            attention_mask = [1] * len(input_ids)
            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))
            attention_mask = attention_mask + [0] * (self.max_seq_len - len(attention_mask))
            x.append(input_ids)
            masks.append(attention_mask)
        return np.array(x), np.array(masks)

tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, "vocab.txt"))
classes = train[IntentDetectionData.LABEL_COLUMN].unique().tolist()

data = IntentDetectionData(train, test, tokenizer, classes, max_seq_len=192)

tokenizer.tokenize("I can't wait to visit Bulgaria again!")

tokens = tokenizer.tokenize("I can't wait to visit Bulgaria again!")
tokenizer.convert_tokens_to_ids(tokens)

classes = train[IntentDetectionData.LABEL_COLUMN].unique().tolist()

from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer
import tensorflow as tf

def create_model(max_seq_len, bert_model_name, num_classes):
    # Load pre-trained BERT model and tokenizer
    model = TFBertForSequenceClassification.from_pretrained(bert_model_name, num_labels=num_classes)
    tokenizer = BertTokenizer.from_pretrained(bert_model_name)

    return model, tokenizer

# Usage
bert_model_name = "bert-base-uncased"
num_classes = len(classes)
model, tokenizer = create_model(data.max_seq_len, bert_model_name, num_classes)

# Prepare the data
def prepare_data(texts, labels, tokenizer, max_seq_len):
    encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_seq_len, return_tensors="tf")
    dataset = tf.data.Dataset.from_tensor_slices((
        dict(encodings),
        labels
    ))
    return dataset

train_dataset = prepare_data(train[IntentDetectionData.DATA_COLUMN].tolist(),
                             [classes.index(label) for label in train[IntentDetectionData.LABEL_COLUMN]],
                             tokenizer,
                             data.max_seq_len)

test_dataset = prepare_data(test[IntentDetectionData.DATA_COLUMN].tolist(),
                            [classes.index(label) for label in test[IntentDetectionData.LABEL_COLUMN]],
                            tokenizer,
                            data.max_seq_len)

# Set up training parameters
batch_size = 16
num_train_steps = (len(train) // batch_size) * 3  # 3 epochs
learning_rate = 2e-5

# Create optimizer
optimizer, lr_schedule = create_optimizer(
    init_lr=learning_rate,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
    num_warmup_steps=0,
)

# Compile the model
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train the model
history = model.fit(
    train_dataset.shuffle(1000).batch(batch_size),
    epochs=3,
    batch_size=batch_size,
)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(test_dataset.batch(batch_size))
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Make predictions and get true labels
true_labels = []
predicted_labels = []

for batch in test_dataset.batch(batch_size):
    inputs, labels = batch
    predictions = model(inputs)
    predicted_batch = tf.argmax(predictions.logits, axis=1)

    true_labels.extend(labels.numpy())
    predicted_labels.extend(predicted_batch.numpy())

true_labels = np.array(true_labels)
predicted_labels = np.array(predicted_labels)

# Print classification report
print(classification_report(true_labels, predicted_labels, target_names=classes))

# For custom predictions
def predict_intent(sentences):
    inputs = tokenizer(sentences, truncation=True, padding=True, max_length=data.max_seq_len, return_tensors="tf")
    outputs = model(inputs)
    predictions = tf.argmax(outputs.logits, axis=1)
    return [classes[pred] for pred in predictions]

# Example usage
sample_sentences = [
    "Halo apa kabar",
    "Rate this book as awful"
]

predicted_intents = predict_intent(sample_sentences)
for sentence, intent in zip(sample_sentences, predicted_intents):
    print(f"Sentence: {sentence}")
    print(f"Predicted Intent: {intent}\n")

"""# References

- https://mccormickml.com/2019/07/22/BERT-fine-tuning/
- https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines
- https://jalammar.github.io/illustrated-bert/
- https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03
- https://www.reddit.com/r/MachineLearning/comments/ao23cp/p_how_to_use_bert_in_kaggle_competitions_a/
"""